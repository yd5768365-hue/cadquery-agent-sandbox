# ğŸ—ï¸ å®Œæ•´ CAE è‡ªåŠ¨åŒ–å¹³å°æ¶æ„

## ğŸ“Š ç³»ç»Ÿæ€»è§ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   å‰ç«¯å±‚ - æ•°å­—å­ªç”Ÿä»ªè¡¨ç›˜                     â”‚
â”‚         Streamlit Dashboard (localhost:8501)               â”‚
â”‚  å®æ—¶ç›‘æ§ | 3Då¯è§†åŒ– | ä»»åŠ¡ç®¡ç† | æ•°æ®åˆ†æ | æ¨¡å‹è®­ç»ƒ       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ä¸­é—´å±‚ - AI äº¤äº’ä¸ä»»åŠ¡è°ƒåº¦                       â”‚
â”‚    Cherry Studio  â†â†’  MCP Server  â†â†’  Task Queue          â”‚
â”‚                         (server.py)    (Celery+Redis)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  è®¡ç®—å±‚ - å®¹å™¨é›†ç¾¤                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Gmsh    â”‚  â”‚ CalculiX â”‚  â”‚  ML/AI   â”‚  â”‚ Visualizeâ”‚  â”‚
â”‚  â”‚ Service  â”‚  â”‚ Service  â”‚  â”‚ Service  â”‚  â”‚ Service  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  æ•°æ®å±‚ - æ™ºèƒ½å­˜å‚¨                            â”‚
â”‚  PostgreSQL  |  Redis  |  FAISSå‘é‡åº“  |  MinIOå¯¹è±¡å­˜å‚¨    â”‚
â”‚  (å…ƒæ•°æ®)     (ç¼“å­˜)      (å‡ ä½•ç‰¹å¾)      (æ–‡ä»¶/ç»“æœ)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ é¡¹ç›®ç›®å½•ç»“æ„

```
E:\DeepSeek_Work\
â”œâ”€â”€ docker/                          # Docker é…ç½®
â”‚   â”œâ”€â”€ docker-compose.yml           # å®¹å™¨ç¼–æ’
â”‚   â”œâ”€â”€ gmsh.Dockerfile              # Gmsh æœåŠ¡
â”‚   â”œâ”€â”€ calculix.Dockerfile          # CalculiX æœåŠ¡
â”‚   â”œâ”€â”€  ml.Dockerfile               # ML æœåŠ¡
â”‚   â””â”€â”€ visualize.Dockerfile         # å¯è§†åŒ–æœåŠ¡
â”‚
â”œâ”€â”€ server/                          # MCP æœåŠ¡å™¨
â”‚   â”œâ”€â”€ server.py                    # ä¸»æœåŠ¡å™¨
â”‚   â”œâ”€â”€ tasks.py                     # Celery ä»»åŠ¡å®šä¹‰
â”‚   â”œâ”€â”€ data_collector.py            # æ•°æ®æ”¶é›†
â”‚   â””â”€â”€ surrogate_model.py           # ä»£ç†æ¨¡å‹
â”‚
â”œâ”€â”€ ml/                              # æœºå™¨å­¦ä¹ æ¨¡å—
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ regression_model.py      # å›å½’é¢„æµ‹å™¨
â”‚   â”‚   â””â”€â”€ geometry_encoder.py      # å‡ ä½•ç‰¹å¾æå–
â”‚   â”œâ”€â”€ trainers/
â”‚   â”‚   â”œâ”€â”€ train_surrogate.py       # è®­ç»ƒä»£ç†æ¨¡å‹
â”‚   â”‚   â””â”€â”€ train_geometry.py        # è®­ç»ƒå‡ ä½•ç¼–ç å™¨
â”‚   â””â”€â”€ data/
â”‚       â””â”€â”€ simulation_history.db    # SQLite æ•°æ®åº“
â”‚
â”œâ”€â”€ dashboard/                       # æ•°å­—å­ªç”Ÿä»ªè¡¨ç›˜
â”‚   â”œâ”€â”€ app.py                       # Streamlit ä¸»ç¨‹åº
â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”œâ”€â”€ monitor.py               # å®æ—¶ç›‘æ§
â”‚   â”‚   â”œâ”€â”€ visualize.py             # 3D å¯è§†åŒ–
â”‚   â”‚   â”œâ”€â”€ analysis.py              # æ•°æ®åˆ†æ
â”‚   â”‚   â””â”€â”€ training.py              # æ¨¡å‹è®­ç»ƒ
â”‚   â””â”€â”€ components/
â”‚       â”œâ”€â”€ charts.py                # å›¾è¡¨ç»„ä»¶
â”‚       â””â”€â”€ 3d_viewer.py             # 3D æŸ¥çœ‹å™¨
â”‚
â”œâ”€â”€ services/                        # å¾®æœåŠ¡è„šæœ¬
â”‚   â”œâ”€â”€ mesh_service.py              # ç½‘æ ¼ç”ŸæˆæœåŠ¡
â”‚   â”œâ”€â”€ solve_service.py             # æ±‚è§£æœåŠ¡
â”‚   â””â”€â”€ viz_service.py               # å¯è§†åŒ–æœåŠ¡
â”‚
â”œâ”€â”€ test/                            # å·¥ä½œç›®å½•
â”‚   â”œâ”€â”€ input/                       # è¾“å…¥æ–‡ä»¶
â”‚   â”œâ”€â”€ parts/                       # æ‹†åˆ†é›¶ä»¶
â”‚   â”œâ”€â”€ meshes/                      # ç½‘æ ¼æ–‡ä»¶
â”‚   â”œâ”€â”€ analyses/                    # åˆ†ææ–‡ä»¶
â”‚   â”œâ”€â”€ results/                     # ç»“æœæ–‡ä»¶
â”‚   â””â”€â”€ visualizations/              # å¯è§†åŒ–å›¾ç‰‡
â”‚
â”œâ”€â”€ config/                          # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ celery_config.py             # Celery é…ç½®
â”‚   â”œâ”€â”€ database_config.py           # æ•°æ®åº“é…ç½®
â”‚   â””â”€â”€ model_config.yaml            # æ¨¡å‹é…ç½®
â”‚
â””â”€â”€ scripts/                         # å·¥å…·è„šæœ¬
    â”œâ”€â”€ setup.sh                     # ä¸€é”®éƒ¨ç½²è„šæœ¬
    â”œâ”€â”€ start_all.sh                 # å¯åŠ¨æ‰€æœ‰æœåŠ¡
    â””â”€â”€ backup.sh                    # æ•°æ®å¤‡ä»½è„šæœ¬
```

---

## ğŸ³ Docker å®¹å™¨ç¼–æ’

### docker-compose.yml

```yaml
version: '3.8'

services:
  # Redis - ä»»åŠ¡é˜Ÿåˆ—å’Œç¼“å­˜
  redis:
    image: redis:7-alpine
    container_name: cae_redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes
    networks:
      - cae_network

  # PostgreSQL - å…ƒæ•°æ®å­˜å‚¨
  postgres:
    image: postgres:15-alpine
    container_name: cae_postgres
    environment:
      POSTGRES_DB: cae_platform
      POSTGRES_USER: cae_user
      POSTGRES_PASSWORD: cae_pass_2024
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - cae_network

  # Gmsh æœåŠ¡
  gmsh-service:
    build:
      context: ./docker
      dockerfile: gmsh.Dockerfile
    container_name: cae_gmsh
    volumes:
      - E:/DeepSeek_Work/test:/app
    environment:
      - SERVICE_NAME=gmsh
      - OMP_NUM_THREADS=4
    networks:
      - cae_network
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 4G

  # CalculiX æœåŠ¡
  calculix-service:
    build:
      context: ./docker
      dockerfile: calculix.Dockerfile
    container_name: cae_calculix
    volumes:
      - E:/DeepSeek_Work/test:/app
    environment:
      - SERVICE_NAME=calculix
      - OMP_NUM_THREADS=8
    networks:
      - cae_network
    deploy:
      resources:
        limits:
          cpus: '8'
          memory: 16G

  # ML æœåŠ¡
  ml-service:
    build:
      context: ./docker
      dockerfile: ml.Dockerfile
    container_name: cae_ml
    volumes:
      - E:/DeepSeek_Work/ml:/app/ml
      - E:/DeepSeek_Work/test:/app/data
    environment:
      - SERVICE_NAME=ml
      - CUDA_VISIBLE_DEVICES=0
    networks:
      - cae_network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # å¯è§†åŒ–æœåŠ¡
  visualize-service:
    build:
      context: ./docker
      dockerfile: visualize.Dockerfile
    container_name: cae_visualize
    volumes:
      - E:/DeepSeek_Work/test:/app
    environment:
      - SERVICE_NAME=visualize
      - DISPLAY=:99
    networks:
      - cae_network

  # Celery Worker
  celery-worker:
    build:
      context: ./server
      dockerfile: Dockerfile
    container_name: cae_celery_worker
    command: celery -A tasks worker --loglevel=info --concurrency=4
    volumes:
      - E:/DeepSeek_Work/server:/app
      - E:/DeepSeek_Work/test:/data
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/0
    depends_on:
      - redis
      - postgres
    networks:
      - cae_network

  # Celery Beat (å®šæ—¶ä»»åŠ¡)
  celery-beat:
    build:
      context: ./server
      dockerfile: Dockerfile
    container_name: cae_celery_beat
    command: celery -A tasks beat --loglevel=info
    volumes:
      - E:/DeepSeek_Work/server:/app
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/0
    depends_on:
      - redis
    networks:
      - cae_network

  # Flower - Celery ç›‘æ§
  flower:
    build:
      context: ./server
      dockerfile: Dockerfile
    container_name: cae_flower
    command: celery -A tasks flower --port=5555
    ports:
      - "5555:5555"
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/0
    depends_on:
      - redis
      - celery-worker
    networks:
      - cae_network

  # Dashboard
  dashboard:
    build:
      context: ./dashboard
      dockerfile: Dockerfile
    container_name: cae_dashboard
    command: streamlit run app.py --server.port=8501
    ports:
      - "8501:8501"
    volumes:
      - E:/DeepSeek_Work/dashboard:/app
      - E:/DeepSeek_Work/test:/data
    environment:
      - DATABASE_URL=postgresql://cae_user:cae_pass_2024@postgres:5432/cae_platform
      - REDIS_URL=redis://redis:6379/0
    depends_on:
      - postgres
      - redis
    networks:
      - cae_network

volumes:
  redis_data:
  postgres_data:

networks:
  cae_network:
    driver: bridge
```

---

## ğŸ”§ æ ¸å¿ƒæ¨¡å—å®ç°

### 1. æ•°æ®æ”¶é›†ç³»ç»Ÿ

```python
# server/data_collector.py
"""
å®Œæ•´çš„æ•°æ®æ”¶é›†ç³»ç»Ÿ
è‡ªåŠ¨è®°å½•æ¯æ¬¡ä»¿çœŸçš„è¾“å…¥å‚æ•°å’Œç»“æœ
"""

import json
import hashlib
from datetime import datetime
from pathlib import Path
import sqlite3
import numpy as np

class SimulationDataCollector:
    def __init__(self, db_path="E:/DeepSeek_Work/ml/data/simulation_history.db"):
        self.db_path = db_path
        self._init_database()
    
    def _init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # ä»¿çœŸè®°å½•è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS simulations (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                sim_id TEXT UNIQUE NOT NULL,
                timestamp TEXT NOT NULL,
                geometry_file TEXT,
                geometry_hash TEXT,
                analysis_type TEXT,
                status TEXT,
                duration REAL
            )
        ''')
        
        # å‡ ä½•å‚æ•°è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS geometry_params (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                sim_id TEXT NOT NULL,
                param_name TEXT NOT NULL,
                param_value REAL NOT NULL,
                FOREIGN KEY (sim_id) REFERENCES simulations(sim_id)
            )
        ''')
        
        # ç½‘æ ¼å‚æ•°è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS mesh_params (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                sim_id TEXT NOT NULL,
                num_nodes INTEGER,
                num_elements INTEGER,
                clmax REAL,
                clmin REAL,
                mesh_quality REAL,
                FOREIGN KEY (sim_id) REFERENCES simulations(sim_id)
            )
        ''')
        
        # ç»“æœè¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS results (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                sim_id TEXT NOT NULL,
                max_stress REAL,
                min_stress REAL,
                mean_stress REAL,
                max_displacement REAL,
                volume REAL,
                mass REAL,
                FOREIGN KEY (sim_id) REFERENCES simulations(sim_id)
            )
        ''')
        
        # å‡ ä½•ç‰¹å¾å‘é‡è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS geometry_features (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                sim_id TEXT NOT NULL,
                feature_vector BLOB NOT NULL,
                feature_dim INTEGER NOT NULL,
                FOREIGN KEY (sim_id) REFERENCES simulations(sim_id)
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def start_simulation(self, geometry_file: str, analysis_type: str, 
                        geometry_params: dict = None) -> str:
        """å¼€å§‹æ–°ä»¿çœŸï¼Œè¿”å› sim_id"""
        
        # ç”Ÿæˆå”¯ä¸€ ID
        sim_id = self._generate_sim_id(geometry_file, geometry_params)
        
        # è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
        geometry_hash = self._hash_file(geometry_file) if Path(geometry_file).exists() else None
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            cursor.execute('''
                INSERT INTO simulations (sim_id, timestamp, geometry_file, 
                                       geometry_hash, analysis_type, status)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (sim_id, datetime.now().isoformat(), geometry_file, 
                 geometry_hash, analysis_type, 'running'))
            
            # è®°å½•å‡ ä½•å‚æ•°
            if geometry_params:
                for name, value in geometry_params.items():
                    cursor.execute('''
                        INSERT INTO geometry_params (sim_id, param_name, param_value)
                        VALUES (?, ?, ?)
                    ''', (sim_id, name, float(value)))
            
            conn.commit()
        except sqlite3.IntegrityError:
            # å·²å­˜åœ¨ç›¸åŒè®°å½•
            pass
        finally:
            conn.close()
        
        return sim_id
    
    def record_mesh(self, sim_id: str, mesh_info: dict):
        """è®°å½•ç½‘æ ¼ä¿¡æ¯"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO mesh_params (sim_id, num_nodes, num_elements, 
                                    clmax, clmin, mesh_quality)
            VALUES (?, ?, ?, ?, ?, ?)
        ''', (sim_id, 
              mesh_info.get('num_nodes'),
              mesh_info.get('num_elements'),
              mesh_info.get('clmax'),
              mesh_info.get('clmin'),
              mesh_info.get('quality', 0.0)))
        
        conn.commit()
        conn.close()
    
    def record_results(self, sim_id: str, results: dict):
        """è®°å½•ä»¿çœŸç»“æœ"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO results (sim_id, max_stress, min_stress, mean_stress,
                               max_displacement, volume, mass)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        ''', (sim_id,
              results.get('max_stress'),
              results.get('min_stress'),
              results.get('mean_stress'),
              results.get('max_displacement'),
              results.get('volume'),
              results.get('mass')))
        
        conn.commit()
        conn.close()
    
    def record_geometry_features(self, sim_id: str, feature_vector: np.ndarray):
        """è®°å½•å‡ ä½•ç‰¹å¾å‘é‡"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # å°† numpy æ•°ç»„è½¬æ¢ä¸º bytes
        feature_bytes = feature_vector.tobytes()
        
        cursor.execute('''
            INSERT INTO geometry_features (sim_id, feature_vector, feature_dim)
            VALUES (?, ?, ?)
        ''', (sim_id, feature_bytes, len(feature_vector)))
        
        conn.commit()
        conn.close()
    
    def complete_simulation(self, sim_id: str, duration: float, status: str = 'completed'):
        """å®Œæˆä»¿çœŸè®°å½•"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            UPDATE simulations 
            SET status = ?, duration = ?
            WHERE sim_id = ?
        ''', (status, duration, sim_id))
        
        conn.commit()
        conn.close()
    
    def get_training_data(self, analysis_type: str = None, limit: int = None):
        """è·å–è®­ç»ƒæ•°æ®"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        query = '''
            SELECT s.sim_id, s.analysis_type,
                   GROUP_CONCAT(gp.param_name || ':' || gp.param_value) as params,
                   mp.num_elements, mp.clmax, mp.clmin,
                   r.max_stress, r.mean_stress, r.max_displacement, r.volume
            FROM simulations s
            LEFT JOIN geometry_params gp ON s.sim_id = gp.sim_id
            LEFT JOIN mesh_params mp ON s.sim_id = mp.sim_id
            LEFT JOIN results r ON s.sim_id = r.sim_id
            WHERE s.status = 'completed'
        '''
        
        if analysis_type:
            query += f" AND s.analysis_type = '{analysis_type}'"
        
        query += " GROUP BY s.sim_id"
        
        if limit:
            query += f" LIMIT {limit}"
        
        cursor.execute(query)
        data = cursor.fetchall()
        conn.close()
        
        return data
    
    def find_similar_simulations(self, geometry_hash: str, top_k: int = 5):
        """æŸ¥æ‰¾ç›¸ä¼¼çš„å†å²ä»¿çœŸ"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT s.sim_id, s.geometry_file, s.timestamp,
                   r.max_stress, r.mean_stress, r.max_displacement
            FROM simulations s
            LEFT JOIN results r ON s.sim_id = r.sim_id
            WHERE s.geometry_hash = ? AND s.status = 'completed'
            ORDER BY s.timestamp DESC
            LIMIT ?
        ''', (geometry_hash, top_k))
        
        results = cursor.fetchall()
        conn.close()
        
        return results
    
    def get_statistics(self):
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        stats = {}
        
        # æ€»ä»¿çœŸæ•°
        cursor.execute("SELECT COUNT(*) FROM simulations")
        stats['total_simulations'] = cursor.fetchone()[0]
        
        # æˆåŠŸç‡
        cursor.execute("SELECT COUNT(*) FROM simulations WHERE status='completed'")
        stats['successful_simulations'] = cursor.fetchone()[0]
        
        # å¹³å‡è€—æ—¶
        cursor.execute("SELECT AVG(duration) FROM simulations WHERE duration IS NOT NULL")
        stats['avg_duration'] = cursor.fetchone()[0]
        
        # æŒ‰ç±»å‹ç»Ÿè®¡
        cursor.execute('''
            SELECT analysis_type, COUNT(*) 
            FROM simulations 
            GROUP BY analysis_type
        ''')
        stats['by_type'] = dict(cursor.fetchall())
        
        conn.close()
        
        return stats
    
    def _generate_sim_id(self, geometry_file: str, params: dict = None) -> str:
        """ç”Ÿæˆå”¯ä¸€ ID"""
        content = f"{geometry_file}_{datetime.now().isoformat()}"
        if params:
            content += json.dumps(params, sort_keys=True)
        return hashlib.md5(content.encode()).hexdigest()[:16]
    
    def _hash_file(self, filepath: str) -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œ"""
        hasher = hashlib.md5()
        with open(filepath, 'rb') as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hasher.update(chunk)
        return hasher.hexdigest()
```

---

## ä¸‹ä¸€æ­¥ï¼šæˆ‘ä¼šåˆ†æ‰¹ç»™ä½ å®Œæ•´ä»£ç 

ç”±äºä»£ç é‡å·¨å¤§ï¼ˆé¢„è®¡ 5000+ è¡Œï¼‰ï¼Œæˆ‘ä¼šåˆ†æˆä»¥ä¸‹éƒ¨åˆ†äº¤ä»˜ï¼š

### ğŸ“¦ äº¤ä»˜æ¸…å•

1. âœ… **æ¶æ„è®¾è®¡å’Œ Docker ç¼–æ’**ï¼ˆå·²å®Œæˆï¼‰
2. âœ… **æ•°æ®æ”¶é›†ç³»ç»Ÿ**ï¼ˆå·²å®Œæˆï¼‰
3. â³ **ç®€å•å›å½’é¢„æµ‹å™¨**
4. â³ **å¯è§†åŒ–å·¥å…·**
5. â³ **ä»»åŠ¡é˜Ÿåˆ—ç³»ç»Ÿ**
6. â³ **å‡ ä½•ç‰¹å¾æå–å™¨**
7. â³ **æ•°å­—å­ªç”Ÿä»ªè¡¨ç›˜**
8. â³ **ä¸€é”®éƒ¨ç½²è„šæœ¬**
